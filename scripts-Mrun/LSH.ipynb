{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimize Hamming Distance calculation - avoid using 2 for loops\n",
    "# Retain nos. of items (change the index in hashed RDD)\n",
    "# Find actual cosine similarity and compare it with the cosine similarity obtained from LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg import Matrices\n",
    "from random import gauss\n",
    "import math\n",
    "\n",
    "np.random.seed(10)\n",
    "count_users = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Original data item : (user, rating)\n",
    "def split_data(x):\n",
    "    sp_x = x.split(\",\")\n",
    "    return int(sp_x[1]), (int(sp_x[0]), float(sp_x[2]))\n",
    "\n",
    "# data = sc.textFile(\"/Users/mrunmayee/AMLProject/Data/train_0_sub_1mil.txt\").repartition(10)\n",
    "# data = sc.textFile(\"/Users/mrunmayee/AdvancedML/Data_AML/netflix_data/test.txt\")\n",
    "data = sc.textFile(\"/Users/mrunmayee/AdvancedML/Data_AML/netflix_data/training_subset.txt\")\n",
    "or_data = data.map(lambda x: split_data(x))\n",
    "#print or_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#s = or_data.reduceByKey(lambda x, y: [x] + [y])\n",
    "#print s.collect()\n",
    "    \n",
    "# m = s.map(lambda x: sort(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert values to list\n",
    "sp_data = or_data.map(lambda x: ((x[0]), ([x[1][0]], [x[1][1]])))\n",
    "# print sp_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to a sparse vector format\n",
    "def conv(x, y):\n",
    "    return x[0] + y[0], x[1] + y[1]\n",
    "\n",
    "sp_format = sp_data.reduceByKey(lambda x, y: conv(x, y))\n",
    "# print sp_format.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1667\n"
     ]
    }
   ],
   "source": [
    "# Count the number of users\n",
    "cn = sp_format.flatMap(lambda x: x[1][0]).distinct().cache()\n",
    "count_users = cn.count()\n",
    "sorted_indices = sorted(cn.collect())\n",
    "\n",
    "\n",
    "print count_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to a sparse vector format by sorting the indices\n",
    "def sort_index(x):\n",
    "    x = list(x)\n",
    "    m = sorted(x[0])\n",
    "    n = [x[1] for (x[0],x[1]) in sorted(zip(x[0],x[1]))]\n",
    "    return m, n\n",
    "    \n",
    "sv = sp_format.mapValues(lambda x: sort_index(x))\n",
    "#sv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create sparse vectors 'sp_vecs' is an RDD of sparse vectors\n",
    "def conv_sv(x):\n",
    "    return Vectors.sparse(count_users, x[1][0], x[1][1])\n",
    "    \n",
    "sp_vecs = sv.map(lambda x: conv_sv(x))\n",
    "# sv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Item ids\n",
    "item_ids = sv.map(lambda x: x[0]).collect()\n",
    "# print item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1667\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Total no. of users = 147371\n",
    "print count_users\n",
    "# Total no. of items = 125951\n",
    "len_items = sp_vecs.count()\n",
    "print len_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function creates random vectors with a norm = 1 and dimension = no. of users\n",
    "def make_rand_vector(dims):\n",
    "    vec = [gauss(0, 1) for i in range(dims)]\n",
    "    mag = sum(x**2 for x in vec) ** .5\n",
    "    return [x/mag for x in vec]\n",
    "\n",
    "\n",
    "# Create a list of vectors\n",
    "list_rand_vecs = []\n",
    "no_vectors = 10\n",
    "for i in xrange(0, no_vectors):\n",
    "    list_rand_vecs.append(Vectors.sparse(count_users, sorted_indices, make_rand_vector(count_users)))\n",
    "\n",
    "def find_hash(x, list_rand_vecs):\n",
    "    global ito, it\n",
    "    ones_zeroes = []\n",
    "    #it += 1\n",
    "    # ito += 1\n",
    "    for i in list_rand_vecs:\n",
    "        if i.dot(x) >= 0:\n",
    "            ones_zeroes.append(1)\n",
    "        else: ones_zeroes.append(0)\n",
    "    \n",
    "    # return (it, ones_zeroes)\n",
    "    return ones_zeroes\n",
    "            \n",
    "\n",
    "hashed = sp_vecs.map(lambda x: find_hash(x, list_rand_vecs)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1, 1, 1, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed.collect()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hashed_in = sc.parallelize(list(enumerate(hashed.collect(), 1)))\n",
    "hashed_in = list(enumerate(hashed.collect(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Hashed values for item 1\n",
    "print hashed_in[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 2), 0.7)\n",
      "((1, 3), 0.5)\n",
      "((1, 4), 0.6)\n",
      "((1, 5), 0.6)\n"
     ]
    }
   ],
   "source": [
    "# Hamming distance gives the probability that the two items are similar\n",
    "def hamming(x,y):\n",
    "    \"\"\"Calculate the Hamming distance between two vectors\"\"\"\n",
    "    assert len(x) == len(y)\n",
    "    c = 0\n",
    "    for i in xrange(0, len(x)):\n",
    "        if x[i] != y[i]:\n",
    "            c += 1\n",
    "    return c * 1.0/ len(x)\n",
    "\n",
    "# Find hamming distance between each pair of items\n",
    "def group_func(ls):\n",
    "    ls_tuples = []\n",
    "    for i in ls:\n",
    "        for j in ls:\n",
    "            if j[0] > i[0]:\n",
    "                ls_tuples.append(((i[0], j[0]), hamming(i[1], j[1])))\n",
    "    return ls_tuples\n",
    "\n",
    "ls_hamming = group_func(hashed_in)\n",
    "for i in xrange(0, 4):\n",
    "    print ls_hamming[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find cosine similarity for all pairs of items\n",
    "import math\n",
    "\n",
    "cosine_sim = []\n",
    "for i in xrange(0, len(ls_hamming)):\n",
    "    cosine_sim.append((ls_hamming[i][0], round(math.cos((1 - ls_hamming[i][1])* math.pi), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 2), 0.5878)\n",
      "((1, 3), 0.0)\n",
      "((1, 4), 0.309)\n",
      "((1, 5), 0.309)\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(0, 4):\n",
    "    print cosine_sim[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionary to store the original item id and the corresponding newly assigned serial no. to it\n",
    "corr_item_ids = {}\n",
    "for i in xrange(0, len_items):\n",
    "    corr_item_ids[item_ids[i]] = i + 1\n",
    "# print corr_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test file\n",
    "testdata = sc.textFile(\"/Users/mrunmayee/AdvancedML/Data_AML/netflix_data/testing_subset.txt\")\n",
    "td = testdata.map(lambda x: split_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert original item ids to the new ones (1,2,....)\n",
    "def dict_conv(x, corr_item_ids):\n",
    "    return (corr_item_ids[x[0]] , (x[1][0], x[1][1]))\n",
    "    \n",
    "conv_td = td.map(lambda x: dict_conv(x, corr_item_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ignore the code after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = Vectors.sparse(sparse_vec.first()[0], sparse_vec.first()[1][0], sparse_vec.first()[1][1])\n",
    "# print sparse_vec.first()[1][1]\n",
    "print m\n",
    "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n",
    "sv1.dot(sp_vecs.take(4)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [\"b\", \"a\", \"d\", \"c\"]\n",
    "y = [3, 2, 1, 4]\n",
    "\n",
    "m = [x for (y,x) in sorted(zip(y,x))]\n",
    "print m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = [4, 3, 2]\n",
    "''.join([str(x) for x in s])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
